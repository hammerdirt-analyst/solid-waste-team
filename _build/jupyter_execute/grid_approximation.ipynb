{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt \n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown as md\n",
    "from myst_nb import glue\n",
    "\n",
    "dfCodes = pd.read_csv(\"resources/data/u_codes.csv\")\n",
    "dfBeaches = pd.read_csv(\"resources/data/u_beaches.csv\")\n",
    "# dfBeaches = dfBeaches[dfBeaches.slug != \"clean-up-event-test\"]\n",
    "dfBeaches.set_index(\"slug\", inplace=True)\n",
    "\n",
    "all_data = pd.read_csv(\"resources/data/u_all_data.csv\")\n",
    "all_data = all_data[all_data.river_bassin != 'les-alpes'].copy()\n",
    "all_data[\"date\"] = pd.to_datetime(all_data[\"date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "# import regional labels. labels are used\n",
    "# to identify the regional priors\n",
    "lac_leman_regions = pd.read_csv(\"resources/data/lac_leman_regions.csv\")\n",
    "\n",
    "# map to code decriptions\n",
    "dfCodes.set_index(\"code\", inplace=True)\n",
    "dfCodes.loc[\"Gcaps\", [\"material\", \"description\", \"groupname\"]] = [\"Plastic\", \"Plastic bottle lids\", \"food and drink\"]\n",
    "code_d = dfCodes[\"description\"]\n",
    "\n",
    "# map to material descriptions\n",
    "mat_d = dfCodes[\"material\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# this defines the css rules for the note-book table displays\n",
    "header_row = {'selector': 'th:nth-child(1)', 'props': f'background-color: #FFF; text-align:right'}\n",
    "even_rows = {\"selector\": 'tr:nth-child(even)', 'props': f'background-color: rgba(139, 69, 19, 0.08);'}\n",
    "odd_rows = {'selector': 'tr:nth-child(odd)', 'props': 'background: #FFF;'}\n",
    "table_font = {'selector': 'tr', 'props': 'font-size: 10px;'}\n",
    "table_data = {'selector': 'td', 'props': 'padding: 12px;'}\n",
    "table_caption = {'selector': 'caption', 'props': 'font-size: 14px; font-style: italic; caption-side: bottom; text-align: left; margin-top: 10px'}\n",
    "table_css_styles = [even_rows, odd_rows, table_font, header_row, table_caption]\n",
    "\n",
    "\n",
    "table_large_data = {'selector': 'tr', 'props': 'font-size: 14px; padding: 12px;'}\n",
    "table_large_font = [even_rows, odd_rows, table_large_data, header_row, table_caption]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Empirical Bayes: grid approximations\n",
    "\n",
    "__Note to colleagues:__\n",
    "\n",
    "This concerns the data from the federal report and a subset of data gathered in 2022. Since the puplication of IQAASL in December 2021 there has been addtional data collection events on Lac Léman:\n",
    "\n",
    "1. SWE team\n",
    "2. Association for the Sauvegarde du Léman [ASL](https://asleman.org/?lang=en) completed 100 beach litter surveys in 2022\n",
    "\n",
    "__Previous associated work, general guidance, big picture:__\n",
    "\n",
    "1. Common sense guidance:\n",
    "   1. The data should be considered as a reasonable estimate of the minimum amount of trash on the ground at the time of the survey.\n",
    "   2. The shoreline is the point where objects abandoned on land enter the water and where objects that are in the water get deposited on the beach.\n",
    "   3. It is necessary to consider the data as a whole. There are many sources of variance. We have treated litter density between sampling groups and the covariance of litter density with topographical features.\n",
    "      1. There are differences between the sampling groups.\n",
    "      2. There are differences between sampling locations\n",
    "      3. There are differences in detect-ability and appearance for items of the same code that are due to the effects of decomposition.\n",
    "      4. Surveyors are volunteers and have different levels of experience or physical constraints that limit what will actually be collected and counted.\n",
    "     \n",
    "```{admonition} If we are using the data to estimate the environmental condition then the response should answer the question:\n",
    "\n",
    "__What and how much are the volunteers likely to find?__\n",
    "      \n",
    "_This is the most honest answer that can be derived from the data. How well the counts perform over time is part of what we are discussing here._\n",
    "```\n",
    "\n",
    "2. Application:\n",
    "   1. __Environmental assessment:__ Where this fits in the environmental assessment process is not clear. For example, when the health of the lake is considered are the findings for litter-data considered alongside bathing water quality? The EU adopted the principle of precaution when the suggested threshold of beach litter was set to 20 pieces of trash for 100 meters [Beach litter thresholds](https://mcc.jrc.ec.europa.eu/main/dev.py?N=41&O=454).\n",
    "   2. __Parameters of interest:__ Considerable effort has been put to exploring the covariance between litter objects and topographical features. There are many positive covariates among objects and topographical features [Near or far](https://hammerdirt-analyst.github.io/landuse/titlepage.html). Suggesting that certain features may be used as parameter values when constructing models. We test this hypothesis when constructing predictions for municipal locations.\n",
    "\n",
    "3. Sampling strategy: \n",
    "   1. Consistent with 1 and given the context in which the samples were collected [Summary test and training data](data-contexts) and because of the work in 2.3 we can see the benefits in sampling many different locations. We have uncertainty about where the 'median' is based on the spread of the sampling statistic but we are not concerned about the geographic spread.\n",
    "   2. The experience with the students demonstrates the importance of small actions. On their own they do not say much about the lake. However, in relation to and combined with the observations with the other groups we have a better idea of what we might find in general and some specific information about Saint Sulpice.\n",
    "\n",
    "__Six year sampling period__\n",
    "\n",
    "The timing of the newest samples, seven years after the first samples were recorded, could be interpreted as the begining of a new six year sampling period that started in January 2022. The Joint Research Center (JRC) at the EU suggest a six year sampling period with preferably ~ 40 surveys in that time, for each beach that is being monitored [Beach litter thresholds](https://mcc.jrc.ec.europa.eu/main/dev.py?N=41&O=454). If these conditions are met a baseline value can be established for the location in question. The baseline value, using this method is the median value of the surveys for the time period.\n",
    "\n",
    "There are over 250 samples from 38 different locations on the lake in the initial six year period. There are no locations that have 40 surveys, therefore the method described previously would not be appropriate for any single location, but it is more than enough for the lake. \n",
    "\n",
    "## Research questions\n",
    "\n",
    "__For the lake and Saint Sulpice:__\n",
    "\n",
    "1. Given the data from 2022, Is there an increase, decrease or no change in the expected survey results given the consolidated results from 2015 - 2021?\n",
    "2. Given the median value for the objects of interest in 2021, what is the chance that a survey in 2022 will exceed this value?\n",
    "3. How do the results from 2022 change the expected survey results going forward?\n",
    "\n",
    "## Practical applications\n",
    "\n",
    "Investments have been made to either prevent or remove litter from the public space. The investments are made with the intention of reducing litter in the environment. The answers to the research questions should help evaluate the return on investment (ROI) from previous projects and provide insights for projects going forward. \n",
    "\n",
    "1. __Did the investment result in a net decline in litter?__\n",
    "2. __What objects were particularly effected?__\n",
    "3. __How does the municpality compare to the rest of the lake?__\n",
    "4. __Where are areas that are in need of the most investment?__\n",
    "\n",
    "## Constraints\n",
    "\n",
    "The assessment method must produce information that directly answers the research question and can be put to practical application immediatley. The data produced should reduce the effort required to produce more specific models.  \n",
    "\n",
    "1. There must be a method to check results integrated into the process.\n",
    "   1. There must be another method that given the same data produces approximately the same results\n",
    "2. The basic calculation should be as simple as possible.\n",
    "   1. By this we mean the definition of the basic calculation should result from a text-book or similar.\n",
    "   2. The prefered level is Maturité Federal or level one calculus\n",
    "   3. The basic calculation should be executable on a spread sheet\n",
    "3. The method must be scalable\n",
    "   1. There should be a path to backend server operations\n",
    "   2. Output formatting should take ML operations into consideration\n",
    "4. Discarding or disregarding data is highly discouraged.\n",
    "\n",
    "## Definitions\n",
    "1. __threshold:__ The pieces of trash per meter of interest. A float value between 0 and 9.99. This represents between 0 and 999 pieces of trash for every 100 meters. Survey values of individual objects rarely exceeded this range.\n",
    "2. __object-code:__ Connects the survey data to information about the category of the object counted. This contains information like material type or intended use. Groups of _object-codes_ can be used to define sources or orgins.\n",
    "3. __frequency:__ The frequency of exceeding a threshold is the number of times that a threshold was exceeded (k) divided by the number of samples taken (n) or k/n.\n",
    "4. __bounding-hex:__ A hexagon inscribed in a circle of r=1500 m with the survey location at the center\n",
    "5. __dry-land:__ The portion of a bounding hex that is not covered by water.\n",
    "6. __land-cover:__ The topographical features within a bounding hex that are common to most survey locations. Land-cover features can occupy op to 100% of available dry land. A bounding hex contains at least one land-cover feature.\n",
    "7. __land-use:__ The topographical features within a bounding hex that are superimposed over the land cover. Land-use features occupy between 0 - 10% of the available dry-land. A bounding hex may or may not contain a land-use feature.\n",
    "8. __event:__ The action of picking up a certain number of pieces of trash, indentifying them and counting them\n",
    "9. __probability:__ The conditional probability $\\theta$ that the number of _events_ will exceed a threshold for a given _object-code_ under the defined conditions of the _bounding-hex_ \n",
    "\n",
    "(assumptions)=\n",
    "## Assumptions\n",
    "\n",
    "1. Locations that have similar environmental conditions will yield similar survey results\n",
    "2. There is an exchange of material (trash) between the beach and body of water\n",
    "3. Following from two, the material recovered at the beach is a result of the assumed exchange\n",
    "4. The type of activities adjacent to the survey location are an indicator of the trash that will be found there\n",
    "5. Following from four and three, the local environmental conditions are an indicator of the local contribution to the mix of objects at the beach\n",
    "6. Surveys are not 100% accurate\n",
    "   1. Some objects will be misidentified\n",
    "   2. Not all objects will be found\n",
    "   3. There will be inaccuracies in object counts or data entry \n",
    "7. Following one through 6: __the survey results are a reasonable estimate of the minimum number of objects that were present at the time the survey was completed__\n",
    "\n",
    "## Test data, training data and objects of interest\n",
    "\n",
    "```{warning}\n",
    "This report does not inculde surveys from the French side of the lake. This is becasue the orginal six years was done using the set of topographical data from Swiss geo admin. To include French locations in this model regional authoritities/experts need to define the appropriate map layers or data sources from France and then a correspondance needs to be established between the two sources.\n",
    "```\n",
    "* __Training data:__ All the survey records on or before May 31, 2021\n",
    "* __Test data:__ All the survey records after after May 31, 2021\n",
    "* __objects of interest:__ The object(s) for which further information is requested. Identified by the object-code.\n",
    "\n",
    "The test and training data is the set of all data collected in Switzerland using the protocol defined in _the guide_ ([Guidance on Monitoring Marine Litter in European Seas](https://publications.jrc.ec.europa.eu/repository/handle/JRC83985)). More specifically: the test and training data are the results of object (trash) counts from individual survey locations over a delimitted length or surface area that is bordered on one side by a lake. _The guide_ suggests a standard length of 100 m of shoreline, this was encouraged but not considered a criteria for exclusion. The minimum recorded survey length of the training data is 5 m, in the test data it is 18 m. The width is measured from the water line to the high-water mark or the physical limits of the beach itself. For the purposes of this study the only minimum length or width for a survey to be valid is that which is imposed by the data itself.\n",
    "\n",
    "### The training data\n",
    "\n",
    "The training data was collected by a variety of organizations over a six year period. In the first sampling campaign (MCBP: 2015-2016) the data from Lake Geneva is primarily from the south part of the lake and collected by two people. Residents of the area would know the region as the _Haut Lac_ with most of the samples coming from the _Riviera_ (agglomermation of Vevey, La Tour-de-Peilz and Montreux). In the second sampling campaign (SLR: 2017-2018) the samples were collected by volunteers from the WWF ([WWF](https://www.wwf.ch/de)). The range extended from the _Haut Lac_ to Gland, including survey locations in Lausanne. \n",
    "\n",
    "The last survey campaign (IQAASL: 2020 - 2021) collected samples from each major region of the lake monthly at fixed locations, other locations were added spontaneously. When the results of SLR were compared to IQAASL a decrease in the number of objects associated with food and tobacco use was considered probable. However, it was unclear if that decline was due to the pandemic restrictions of 2020, [Conclusion SLR v/s IQAASL](https://hammerdirt-analyst.github.io/IQAASL-End-0f-Sampling-2021/slr-iqaasl.html#fazit). \n",
    "\n",
    "### The test data\n",
    "\n",
    "The test data is a combination of the data collected by the ASL and the SWE team. Plastock is a project run by the ASL between January and December 2022. They conducted 95 beach liter surveys, from 25 different locations ([plastock](https://hammerdirt-analyst.github.io/plastock/)). The data was analyzed in partnership with the project manager from the ASL to determine suitability for this study. The protocol for plastock was based off of the national survey protocol ([IQAASL](https://hammerdirt-analyst.github.io/IQAASL-End-0f-Sampling-2021/land_use_correlation.html)), the collection and identification was completed by volunteers.\n",
    "\n",
    "The survey dimensions in 2022 (test data) were on average longer 69 m v/s 48 m and wider 430 m² v/s 209 m² than the training data. There are a total of 245 samples in the training data, this is all the data collected in the first six year sampling period. There are 99 samples in the test data, 95 samples from the ASL and 4 samples from SWE. The test and training data are described by seven columns: *loc_date (location and date)*, _location_, _date_, _day of year (doy)_, _project (testing or training)_, _code (object code)_, _pcs/m (pieces per meter)_. \n",
    "\n",
    "(objects-of-interest)=\n",
    "### The objects of interest\n",
    "\n",
    "From the 2021 report there are 230 object-codes that can be attributed to each one of the 384 surveys. Some objects were found and counted only once, such as paint brushes (G166) others were found in 87% of all samples, cigarette ends (G27). The 15 most abundant objects from Lake Geneva indentified in IQAASL account for 75% of all the objects counted that year [Lake Geneva IQAASL](https://hammerdirt-analyst.github.io/IQAASL-End-0f-Sampling-2021/lac-leman.html), table one. There are some exceptions that must be eliminated, and explained:\n",
    "\n",
    "1. Nurdles or injection molding pellets were not counted prior to 2020\n",
    "2. Plastock was focussed on plastic objects\n",
    "\n",
    "The codes of interest are selected from the 15 most abundant objects from the federal report of 2021 AND specific objects that were counted less often but are relativeley easy to identify. Furthermore, we only consider the objects that were also identified in the testing data. \n",
    "\n",
    "A surveyor is likely to encounter common objects in the various states of fragmentation or decomposition. Objects that are easy to identify or to describe have a better chance a being placed under the correct object code. For example a cigarette end is immediately recognizable. Fragmented or otherwise degraded objests are challenging, determining wether or not a plastic bottle cap comes from a beverage or a chemical container can be difficult when all the labeling is removed or eroded. From the original data the following object-codes were aggregated into groups:\n",
    "\n",
    "1. Gfoam: Fragmented expanded polystyrene, object-codes: G81, G82, G83\n",
    "2. Gfrags: Fragmented plastics, object-codes: G78, G79, G80, G75, G76, G77\n",
    "3. Gcaps: Plastic bottle lids and lid rings, object-codes: G21, G23, G24\n",
    "\n",
    "Note that aggregating object codes into groups is a common strategy. When evaluating litter densities in the marine environment _Single Use Plastics_ (SUP's) is a common group that contains the objects like plastic bottles or disposable food containers another common group is _fishing gear_ [Beach litter thresholds](https://mcc.jrc.ec.europa.eu/main/dev.py?N=41&O=454). There are 16 objects of interest for this initial study (including the three aggregated groups). These objects represent different use-cases and sources. It is these use cases we will evaluate.\n",
    "\n",
    "1. __Personal hygiene (Ph)__, spatial source: diffuse, toilets, water treatment facilities\n",
    "   1. G95: cotton swabs\n",
    "2. __Personal consumption (Pc)__, spatial source: local to survey location, abandoned within 1 500 m of the survey\n",
    "   1. G30: Snack wrappers\n",
    "   2. Gcaps: drink bottles, caps and lid rings\n",
    "   3. G10: To go containers\n",
    "   4. G25: Tobacco related, not cigartetts\n",
    "   5. G27: cigarette ends\n",
    "   6. G35: Straws and stirrers\n",
    "   7. G31: Lollypop sticks\n",
    "   8. G32: Toys, party favors\n",
    "   9. G33: Lids for to go drinks\n",
    "3. __Industrial/professional (Ip)__, spatial source: diffuse and local, transported to survey location or professional activities within 1 500 meters of survey location \n",
    "   1. G67: Plastic sheeting\n",
    "   2. G89: Construction plastics\n",
    "   3. Gfoam: Fragmented expanded polystyrene\n",
    "4. __Unknown (Unk)__, spatial source: diffuse and local, transported to survey location or professional activities within 1 500 meters of survey location \n",
    "   1. Gfrags: Fragmented plastics\n",
    "5. __Recreation/sports (Rc)__, spatial source: diffuse, transported to survey location, it is illegal to discharge firearms on the lakeshore\n",
    "   1. G70: Shotgun shells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "cois = cities_of_interest = ['Saint-Sulpice (VD)', 'Saint Gingolph', 'Genéve', 'Cully', 'Vevey']\n",
    "toi = trash_of_interest = ['Gfrags', 'G30', 'G27', 'Gfoam', 'G95', 'G144', 'G98','Gcaps', 'G67', 'G35', 'G89', 'G31', 'G32', 'G33', 'G25', 'G70', 'G10']\n",
    "some_quants = [.03, .25, .48, .5, .52, .75, .97]\n",
    "end_training_date = \"2021-05-31\"\n",
    "begin_training_date = \"2015-11-15\"\n",
    "\n",
    "use_groups =  {\n",
    "    'Personal hygiene':['G95', 'G100'],\n",
    "    'Personal consumption':[\n",
    "    'G30', 'Gcaps', 'G10', 'G25', 'G27', 'G35', 'G31', 'G32', 'G33'],\n",
    "    'Industrial/professional': ['G67', 'G89', 'Gfoam'],\n",
    "    'Unknown':['Gfrags'],\n",
    "    'Recreation/sports': ['G70']\n",
    "}\n",
    "\n",
    "use_groups_i =  {\n",
    "    'G95':'Personal hygiene',\n",
    "    'G100':'Personal hygiene', \n",
    "    'G30':'Personal consumption',\n",
    "    'Gcaps':'Personal consumption',\n",
    "    'G10':'Personal consumption',\n",
    "    'G25':'Personal consumption',\n",
    "    'G27':'Personal consumption',\n",
    "    'G35':'Personal consumption',\n",
    "    'G31':'Personal consumption',\n",
    "    'G32':'Personal consumption',\n",
    "    'G33':'Personal consumption',\n",
    "    'G144':'Personal hygiene',\n",
    "    'G98': 'Personal hygiene',\n",
    "    'G67':'Industrial/professional',\n",
    "    'G89':'Industrial/professional',\n",
    "    'Gfoam':'Industrial/professional',\n",
    "    'Gfrags':'Unknown',\n",
    "    'G70':'Recreation/sports'\n",
    "}\n",
    "\n",
    "abbrev_use_g = {'Unknown':'Unk','Personal consumption':'Pc', 'Personal hygiene': 'Ph',    'Recreation/sports': 'Rc', 'Industrial/professional':'Ip'}\n",
    "\n",
    "lake = 'lac-leman'\n",
    "\n",
    "not_these = ['amphion', 'anthy', 'excenevex', 'lugrin', 'meillerie', 'saint-disdille', 'tougues']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "def prior_distributions(prior_data: pd.DataFrame = None, start: str = None, end: str = None,\n",
    "                        xrange: np.array = None, uninformed_prior: np.array = None):\n",
    "    data_args = {\n",
    "        'start':start,\n",
    "        'end':end,\n",
    "        'data': prior_data,\n",
    "    }\n",
    "    prior_pcs = period_pieces(*data_args.values())         \n",
    "\n",
    "    # get n and k for the prior data\n",
    "    prior_k, prior_notk, prior_k_n_minus_k = period_k_and_n(prior_pcs, xrange)\n",
    "   \n",
    "    # make the likelihood parameters\n",
    "    lhx = list(zip(prior_k, prior_notk))\n",
    "\n",
    "    # make the prior distribution\n",
    "    p_ui, prior_bmean = make_expected(lhx, uninformed_prior, xrange)\n",
    "\n",
    "    # the uninformed beta approximation of the prior data\n",
    "    prior_beta = [period_beta(x) for x in prior_k_n_minus_k]\n",
    "    p_beta= [x.mean() for x in prior_beta]\n",
    "\n",
    "    results=pd.DataFrame({\"x\":xrange, \"p\":p_ui})\n",
    "    results[\"pn\"] = results.p/results.p.sum()\n",
    "    \n",
    "    return np.array(p_ui), np.array(p_beta), prior_k_n_minus_k, results, prior_pcs\n",
    "\n",
    "def posterior_distribution(lh_data: pd.DataFrame = None, start: str = None, end: str = None, code: str = None,\n",
    "                           informed_prior: np.array = None, un_informed: np.array = None):\n",
    "                               \n",
    "    \n",
    "    data_args = {\n",
    "        'start': start,\n",
    "        'end':end,\n",
    "        'data': lh_data,   \n",
    "        }\n",
    "\n",
    "    period_all = period_pieces(*data_args.values())\n",
    "    \n",
    "    pall_k, pall_notk, pall_k_n_minus_k = period_k_and_n(period_all, xrange)\n",
    "    \n",
    "    lh_and_informed = np.array(pall_k_n_minus_k) + np.array(informed_prior)\n",
    "    lhx = list(zip(pall_k, pall_notk))        \n",
    "    \n",
    "    probi, probi_beta = make_expected(pall_k_n_minus_k, np.array(informed_prior), xrange)\n",
    "    grid_prox, grid_prox_beta = make_expected(pall_k_n_minus_k, un_informed, xrange)\n",
    "    \n",
    "    # beta distribution \n",
    "    pall_beta = [period_beta((x[0]+1, x[1]+1)) for x in pall_k_n_minus_k]\n",
    "    pall_bmean = [x.mean() for x in pall_beta]\n",
    "    return np.array(probi), np.array(grid_prox), pall_bmean, period_all\n",
    "                               \n",
    "def training_testing_compare(lh_pcs, pcs, post_quants, prior_quants):\n",
    "    \n",
    "    total_training = len(pcs) + len(lh_pcs)\n",
    "    prior_weight = len(pcs)/total_training\n",
    "    lh_weight = len(lh_pcs)/total_training\n",
    "\n",
    "    number_of_samples = {\"Training\": len(pcs), \"Testing\": len(lh_pcs)}\n",
    "    weights = {\"Training\":prior_weight, \"Testing\": lh_weight}\n",
    "    observed_median = {\"Training\":np.median(pcs), \"Testing\": np.median(lh_pcs)}\n",
    "    observed_average = {\"Training\":np.mean(pcs), \"Testing\": np.mean(lh_pcs)}\n",
    "    observed_25 = {\"Training\": prior_quants[1], \"Testing\":post_quants[1]}\n",
    "    observed_75 = {\"Training\": prior_quants[5], \"Testing\":post_quants[5]}\n",
    "    index = [\"weight all samples\", \"Number of samples\", \"Median\", \"Average\", \"25th percentile\", \"75th percentile\"]\n",
    "    components = [weights, number_of_samples, observed_median, observed_average, observed_25, observed_75]\n",
    "    unks_sum_table = pd.DataFrame(components, index=index).style.format(precision=2).set_table_styles(table_large_font)\n",
    "    styled = unks_sum_table.format(formatter=\"{:.0f}\", subset=pd.IndexSlice[['Number of samples'], :])\n",
    "    \n",
    "    return styled\n",
    "\n",
    "def predicted_summary(lh_pcs, pcs, prior_quants, median_2024):\n",
    "    \n",
    "\n",
    "    predicted = ((lh_pcs <= prior_quants[5])&(lh_pcs >= prior_quants[1])).sum()/len(lh_pcs)\n",
    "    predicted_94 = ((lh_pcs <= prior_quants[-1])&(lh_pcs >= prior_quants[0])).sum()/len(lh_pcs)\n",
    "    past_present_future = {\n",
    "        \"Median 2021\": np.median(pcs), \n",
    "        \"Median 2022\": np.median(lh_pcs), \n",
    "        \"Expected sampling median 2024\":median_2024,\n",
    "        \"% 2022 in 50% IQR  predicted\": predicted,\n",
    "        \"% 2022 in 94% IQR  predicted\": predicted_94,\n",
    "    }\n",
    "        \n",
    "    \n",
    "    ppf = pd.DataFrame(past_present_future, index=[\"pcs/m\"]).T\n",
    "\n",
    "    return ppf\n",
    "\n",
    "\n",
    "def make_results_df(prior_df, lh_c, source=None, source_norm=None):\n",
    "    prior_df[source] = lh_c\n",
    "    prior_df[source_norm] = prior_df[source]/prior_df[source].sum()\n",
    "\n",
    "    return prior_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(data-contexts)=\n",
    "### Summary test and training data\n",
    "\n",
    "Another way to look at this collection of observations is that each sampling group collected the data for reasons that were specific to that group, the protocol provided a framework for ensuring consistency and a pathway to interpreting the results. However, this does not mean that each group interpreted the protocol in the same manner, nor does it mean that all objects collected were counted. By limiting analysis to specific object-codes, those that appear most often and/or those that are easily identified, uncertainty is reduced by leveraging frequency of occurence and domain experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "cbdi = pd.read_csv(\"resources/data/u_pstk_iqaasl_all.csv\")\n",
    "cbd = cbdi[cbdi.code.isin(toi)]\n",
    "\n",
    "column_names_groups = {v:k for k,v in abbrev_use_g.items()}\n",
    "code_groups = list(column_names_groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "summ_data = cbd.copy()\n",
    "summ_data = summ_data[~summ_data.slug.isin(not_these)]\n",
    "summ_data[\"use group\"] = summ_data.code.map(lambda x: use_groups_i[x])\n",
    "\n",
    "\n",
    "summ_data[\"ug\"] = summ_data[\"use group\"].apply(lambda x: abbrev_use_g[x])\n",
    "summ_data[summ_data[\"use group\"] == 'Personal consumption'].code.unique()\n",
    "summ_data[\"date\"] = pd.to_datetime(summ_data[\"date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "sd_x = summ_data.groupby([\"loc_date\", \"date\", \"city\", \"Project\", \"doy\"], as_index=False).agg({\"pcs/m\": 'sum', 'quantity':'sum'})\n",
    "\n",
    "\n",
    "trg = summ_data[summ_data.Project == \"Training\"].copy()\n",
    "tst = summ_data[summ_data.Project == \"Testing\"].copy()\n",
    "trg_c, tst_c = trg.city.nunique(), tst.city.nunique()\n",
    "trg_lc, tst_lc = trg.slug.nunique(), tst.slug.nunique()\n",
    "trg_q, tst_q = trg.quantity.sum(), tst.quantity.sum()\n",
    "\n",
    "data_magnitude = [\n",
    "    {\"Training\":trg_c, \"Testing\":tst_c},\n",
    "    {\"Training\":trg_lc, \"Testing\":tst_lc},\n",
    "    {\"Training\":trg_q, \"Testing\":tst_q}\n",
    "    \n",
    "]\n",
    "\n",
    "cities_set = list(set([*trg.city.unique(), *tst.city.unique()]))\n",
    "n_ind_cities = len(cities_set)\n",
    "\n",
    "caption = f'The number of different locations and cities for the data. Note that there are {n_ind_cities} different municipalitites in all.'\n",
    "\n",
    "data_summ_q = pd.DataFrame(data_magnitude, index=[\"Number of cities\", \"Number of locations\", \"Total objects\"]).astype('int')\n",
    "data_summ_q = data_summ_q.style.format(formatter=\"{:,}\").set_table_styles(table_large_font).set_caption(caption)\n",
    "styled = data_summ_q.format(formatter=\"{:,}\", subset=pd.IndexSlice[['Total objects'], :])\n",
    "glue(\"data-summ-q\", styled, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# all the data by date\n",
    "the_99th_percentile = np.quantile(sd_x['pcs/m'].values, .99)\n",
    "px = 1/plt.rcParams['figure.dpi']  # pixel in inches\n",
    "fig, ax = plt.subplots(figsize=(600*px,500*px))\n",
    "\n",
    "sns.scatterplot(data=sd_x, x='date', y='pcs/m', hue='Project', hue_order=[\"Training\", \"Testing\"],ax=ax)\n",
    "\n",
    "ax.set_ylim(-1, the_99th_percentile)\n",
    "ax.set_xlabel(\"\")\n",
    "glue(\"testing_training_chrono\", fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# all the data day of year\n",
    "fig, ax = plt.subplots(figsize=(600*px, 500*px))\n",
    "\n",
    "sns.scatterplot(data=sd_x, x='doy', y='pcs/m', hue='Project',  hue_order=[\"Training\", \"Testing\"],ax=ax)\n",
    "ax.set_ylim(-1, the_99th_percentile)\n",
    "ax.set_xlabel(\"Day of the year\")\n",
    "glue('testing_training_doy', fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "testing_vals= sd_x[sd_x.Project == \"Testing\"]['pcs/m'].values\n",
    "training_vals = sd_x[sd_x.Project == \"Training\"]['pcs/m'].values\n",
    "\n",
    "\n",
    "train_quantiles = np.quantile(training_vals, some_quants)\n",
    "test_quantiles = np.quantile(testing_vals, some_quants)\n",
    "\n",
    "training_testing_summary = training_testing_compare(testing_vals, training_vals, test_quantiles, train_quantiles)\n",
    "caption = \"The observed values from the training and testing data. Remark that the testing data is only 22% of all the data. This is because we are only in the first year of a six year sampling period\"\n",
    "sum_table = training_testing_summary.set_caption(caption)\n",
    "sum_table.format(formatter=\"{:.0f}\", subset=pd.IndexSlice[['Number of samples'], :])\n",
    "glue(\"data-summary\", sum_table, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(600*px, 500*px))\n",
    "\n",
    "sns.ecdfplot(data=sd_x, x='pcs/m', hue='Project',  hue_order=[\"Training\", \"Testing\"],ax=ax)\n",
    "ax.set_xlim(-1, the_99th_percentile)\n",
    "ax.set_ylabel(\"Cumulative probability\")\n",
    "glue('testing_training_cumulative', fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "|Figure 1, Table 1 | Table 2, Figure 3|\n",
    "|:-----------------------:|:---------------------:|\n",
    "|{glue:}`testing_training_chrono` |{glue}`data-summary`|\n",
    "|{glue:}`data-summ-q`|{glue}`testing_training_doy`|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Methods\n",
    "\n",
    "The research questions and practical applications are inquiring about expected results at the municipal level. There are records for 25 municipalities on Lake Geneva, some only have one sample in the entire sampling period. The negative binomial distribution was used to model expected survey results at the river bassin and national level [Estimating baselines IQAASL](https://www.plagespropres.ch/baselines.html#calculating-baselines). Here we are abandoning the assumption that the data has a particular shape and solving the binomial portion of the negative binomial distribution at each interval on the set of numbers from 0 - 10, every 0.01.\n",
    "\n",
    "We use conditional probability because of the assumptions of our model. In this sense we are following trends from the conservationists and wildlife biologists. Both fields have a rich history of treating observations from the field that originate from citizen science projects and/or difficult field sampling conditions. Beach litter data collection is one such program. ([summarizing bird counts](https://www.fs.usda.gov/psw/publications/documents/psw_gtr191/psw_gtr191_0762-0770_sauer.pdf), [estimating age of survival](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.14077), [estimating tick abundance](https://hal.science/hal-02637100/)). The magnitude of the exchange between the water source and the beach is yet another variable that is for the most part unknown, except that which can be interpreted from the survey data ([Identifying Marine Sources of Beached Plastics](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2021GL097214)). In summary, there are many sources of variance, only one of which is the sampling error.\n",
    "\n",
    "The applied method would best be classified as Empirical Bayes, in the sense that the prior is derived from the data ([Bayesian Filtering and Smoothing](https://users.aalto.fi/~ssarkka/pub/cup_book_online_20131111.pdf), [Empirical Bayes methods in classical and Bayesian inference](https://link.springer.com/article/10.1007/s40300-014-0044-1)). However, we share the concerns of Davidson-Pillon ([Bayesian methods for hackers](https://dataorigami.net/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/#contents)) about double counting and eliminate it as part of the formulation of the prior. This is possible because of the number of different locations and regions that were sampled durring the sampling period. The basis for this method was originally explored in the Swiss federal report and then again with much more rigor in [Near or far](https://hammerdirt-analyst.github.io/landuse/titlepage.html).\n",
    "\n",
    "```{note}\n",
    "Model types or analytical labels are only important if the context is understood. Empirical Bayes means we are building a probabilistic model that uses existing data to establish those probabilities.\n",
    "```\n",
    "\n",
    "### Grid Approximation\n",
    "\n",
    "Grid approximations are made from a series of point estimates calculated using Bayes theorem. Condtional probability and Bayes theorem makes it possible to measure the magnitude of an unknown parameter as long as the conditions can be quantified. There is no assumption about the underlying relationships between variables except that a realtionship exists. The relationship is defined in the notation: _the probability of a given b_ or $p(a|b)$. ([Statistical Machine Learning](https://www.stat.cmu.edu/~larry/=sml/Bayes.pdf) [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)) ([conditional proabability](https://en.wikipedia.org/wiki/Conditional_probability)).\n",
    "\n",
    "The proposed model only demands whether or not a threshold has been exceeded. This is a binary variable. Therfore each step in the grid can be modeled using the binomial distribution ([Think Bayes 2](https://allendowney.github.io/ThinkBayes2/chap02.html), [Bayes Rules! An Introduction to Applied Bayesian Modeling](https://www.bayesrulesbook.com/chapter-6.html)). The prior data can be introduced and the integral can be solved analytically by using the prior conjugate of the binomial([Bayesian methods for hackers](https://dataorigami.net/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/#contents),[Prior Probabilities](https://bayes.wustl.edu/etj/articles/prior.pdf)). The grid we are covering is relatively small, 1000 points, but it does represent real values in pieces of trash per meter between 0 and 10. This accounts for 99% of the data in the previous section.\n",
    "\n",
    "```{note}\n",
    "A grid approximation is not the same as a hierarchical model. The grid approximation is away to explore the suitability of the method with the data, in relation to the research question without the overhead of developing a full model. Because of the compromise a grid approximation is less accurate than a hierarchical model.\n",
    "```\n",
    "\n",
    "### Conditional probability\n",
    "\n",
    "Conditional probability is the measure of the probability of an event occuring given that another event has occurred ([Wikepedia](https://en.wikipedia.org/wiki/Conditional_probability)). For this study the event under consideration is whether or not a threshold was exceeded. The probability of that event is noted $p(\\theta)$, the probability of $\\theta$ given a condition or set of condtions is $p(\\theta | condition(s))$. \n",
    "\n",
    "__The general case__\n",
    "\n",
    "Using the formal definition of conditional probability let a = $\\theta$ and b = _event data_, the probability of an event given a condtion is:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "p(a | b) =&\\ \\frac{p(a \\cap b)}{p(b)},\\ 0.0 \\lt a \\lt 1,\\ 0.0 \\lt b \\lt 1 \\tag{1}\\\\[10pt]\n",
    "and\\\\\n",
    "p(b | a) =&\\ \\frac{p(b \\cap a)}{p(a)},\\ p(a \\cap b) =\\ p(b \\cap a) \\tag{2}\\\\[10pt]\n",
    "therefore\\\\\n",
    "p(b | a) =&\\ \\frac{p(b \\cap a)}{p(a)}\\ =\\ \\frac{p(a \\cap b)}{p(a)} \\tag{3}\\\\[10pt]\n",
    "p(a)p(b|a) =&\\ p(a \\cap b) \\tag{4}\\\\[10pt]\n",
    "finally\\\\\n",
    "p(a | b) =&\\ \\frac{p(a)p(b|a)}{p(b)} \\tag{5}\\\\[10pt]\n",
    "\\text{total probability = evidence} =&\\ p(b) =\\ p(b|a)p(a) + p(b|a^{\\prime})p(a^{\\prime}) \\tag{6}\\\\[11pt]\n",
    "posterior =&  \\frac{likelihood*prior}{evidence}\\tag{7}\\\\[10pt]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is Bayes theorem, it tells us that the probability of event _a_ is conditioned on event _b_. If the sample space can be defined by $a$ and $a^{\\prime}$ then the total probability is the sum of the (likelihood * the prior) and (the complement to the prior*the complement to the likelihood). This means that we consider only two possible results: $y \\ge x$ or $y \\lt x$, where x is a threshold value and y is a survey result in pieces per meter (pcs/m). To use Bayes theorem we need to assign values to the likelihood and prior and cary out the math in (7).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{prior} =& \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} * \\theta^{k} (1-\\theta)^{n_{pr} - k}, \\ \\ \\text{beta distribution} \\tag{8} \\\\[12pt]\n",
    "\\text{likelihood} =& {n\\choose s}*\\theta^{s}(1-\\theta)^{n-s},\\ \\ \\text{binomial distribution} \\tag{9} \\\\[12pt]\n",
    "\\text{prior*likelihood} =& C*\\theta^{s+a}*(1-\\theta)^{n+b-k-a},\\ \\ C=\\frac{\\Gamma(a + b + n)}{\\Gamma(s+a)\\Gamma(n - s + b)} \\tag{10} \\\\[12pt]\n",
    "\\text{evidence} =& \\int_{i=0}^1 \\theta*\\theta^{s+a}*(1-\\theta)^{n+b-k-a} d\\theta\\ = \\frac{\\Gamma(s+a)\\Gamma(n - s + b)}{\\Gamma(a + b + n)} = 1 \\tag{11} \\\\[12pt] \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "```{important}\n",
    "\n",
    "The beta distribution (8) is the prior conjugate to the binomial distribution (9) ([Statistical Machine Learning](https://www.stat.cmu.edu/~larry/=sml/Bayes.pdf), [Conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior)). This means that the posterior distribution can be solved analytically. The evidence (11) which is an integral can be solved by recognizing that (11) is another way to write the beta distribution which integrates to one.\n",
    "\n",
    "The mean (average) of the beta distribution above is $\\frac{a + s}{a + b + n}$, which is the value that is calculated for each interval in the grid approximation.\n",
    "```\n",
    "\n",
    "Therefore we can write the solution of Bayes theorem using the beta binomial conjugate model as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(a|b) \\sim & \\ Beta(s+a, n+b-s) \\tag{12} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### The priors\n",
    "\n",
    "The priors used in this model are _subjective_, this is by definition because they come from the data ([Bayesian Filtering and Smoothing](https://users.aalto.fi/~ssarkka/pub/cup_book_online_20131111.pdf)). The subjecitve bias is caused by the use of data from similar experiments that were carried out under different conditions at similar locations. However, we omit the location(s) of interest from the prior data. Instead we rely on other locations that have similar a land-use configuration as the locations of interest. \n",
    "\n",
    "This use of the prior fits well with the initial assumptions of the model and the previous work using Spearmans $\\rho$ to indentify covariates between objects and topographical features ([Near or far](https://hammerdirt-analyst.github.io/landuse/titlepage.html)). This increases the amount of available data for any single location. A distance component is used to capture the locations closest to the locations of interest. Regional denominations work well also. In this model the three regions of the lake are all considered subsets of prior data. ([Empirical Bayes methods in classical and Bayesian inference](https://link.springer.com/article/10.1007/s40300-014-0044-1))\n",
    "\n",
    "#### The informed prior\n",
    "\n",
    "An informed prior is a collection of results from different locations that have smilar magnitudes of specific land-use attributes. An informed prior does not contain survey results from the components of the likelihood. It is located at least within the same river bassin and most often on the same body of water as the likelihood component. This can be made even more explicit:\n",
    "\n",
    "1. informed prior: The probability of x described by a subset of the data that was observed at a date on or before the maximum date of the likelihood data being evaluated.\n",
    "2. The initial evaluation should be a covariance test such as Spearmans $\\rho$ or other\n",
    "   1. The subset of data is related to the observed data either geographically or by a measurable attribute\n",
    "      1. example of geographic relationship: same lake or river basin\n",
    "      2. example of measurable attribute: similar land use configuration\n",
    "         1. samples within 5% of each other on the magnitude scale for buildings\n",
    "         2. the samples with the lowest percentile ranking for infrastructure\n",
    "\n",
    "It may be benficial to note that this prior satisfies the condition of testability. Even though we do not know exactly what the prior distribution will be at any moment the conditions imposed give us an expected result based on experiences accrued elsewhere under similar conditions. In that sense we remain consistent with our assumptions and testable in the sense that the priors are quantifiable ([Prior Probabilities](https://bayes.wustl.edu/etj/articles/prior.pdf)).\n",
    "\n",
    "The set of available data is small and choices or statements about the value of the $prior$ have considerable weight in reference to the posterior distribution. Having multiple possible values for the $prior$ is consistent with Bayesian analysis.([Gelman prior distribution](http://www.stat.columbia.edu/~gelman/research/published/p039-_o.pdf)). \n",
    "\n",
    "#### The uninformed prior\n",
    "\n",
    "The uninformed prior is the initial prior we used, whch captures the expectations of most beach litter surveyors: _you can find anything and if you do it long enough you will_. The uninformed prior is the distribution such that every value of x on the described interval has an equal chance of occuring: 0.5 or 1/2 or even 50%. We use this prior when their is no source of prior data. For example, for Lake Geneva there is no other source of comparable data in the river bassin.\n",
    "\n",
    "These values are exchanged in the form of the coeficients $k, n-k$ in (8). The value that is in the grid is the mean of the binomial distribution of the probability of exceeding the threshold at that point. The grid has 1000 points. Having multiple possible values for the $prior$ is analagous with looking at the problem from different angles or changing some baseline assumptions [Our assumptions](assumptions) . \n",
    "\n",
    "#### The measured land-use attributes\n",
    "\n",
    "```{important}\n",
    "There are slight changes in the way the land-use variables are handled and described with respect to [Near or far](https://hammerdirt-analyst.github.io/landuse/titlepage.html). Notably the infrastruture and recreation variables are scaled separately from the land-cover variables.\n",
    "\n",
    "The land-use attributes are described here in detail. The descriptions are issue from the map themselves, these are easy to integrate into other geo maps from the swiss admin system. Here we are letting go of some of the control and limitting our varaibles to a set of choices. Those choices were derived by experts, they are certainly better at classifying land use than us.\n",
    "```\n",
    "The informed priors in this study are assembled by considering the survey results from locations that have similar environmental conditions. The connection between measurable land-use attributes and survey results was illustrated in the swiss national survey ([IQAASL](https://hammerdirt-analyst.github.io/IQAASL-End-0f-Sampling-2021/land_use_correlation.html)) and explored in depth, including early versions of the proposed model in [Near or far](https://hammerdirt-analyst.github.io/landuse/titlepage.html). The source of the land-use data is [swissTLMRegio](https://www.swisstopo.admin.ch/de/geodata/landscape/tlmregio.html), the details of extracting the data and defining the boundary conditions can be found here [New map data](https://hammerdirt-analyst.github.io/landuse/hex-3000-m.html).\n",
    "\n",
    "__land-cover__\n",
    "\n",
    "These measured land-use attributes are the labeled polygons from the map layer _Landcover_ defined here ([swissTLMRegio product information](https://www.swisstopo.admin.ch/fr/geodata/landscape/tlm3d.html#dokumente)), they are extracted using vector overlay techniques in QGIS ([QGIS](https://qgis.org/en/site/)). The overlay is a hexagon-grid, each hex is 3000m, circumcscribed by a circle r=1500m. The survey location is located at the center of the hex. The magnitude of the land-use variable is the portion of the total dry surface area for any particular land-use attribute. Areas of the hex that are not defined with a land-use attribute in this map layer are labeled _undefined_ and processed like any other land-use attribute. The land-cover variables of interest are:\n",
    "\n",
    "1. Buildings: built up, urbanized\n",
    "2. Woods: not a park, harvesting of trees may be active\n",
    "3. Vineyards: does not include any other type of agriculture\n",
    "4. Orchards: not vineyards\n",
    "5. Undefined: areas of the map with no predefined label\n",
    "\n",
    "__Land-use__\n",
    "\n",
    "Land-use variables are the labled polygons from the _Freizeitareal_ and  _Nutzungsareal_ map layers, defined in ([swissTLMRegio product information](https://www.swisstopo.admin.ch/fr/geodata/landscape/tlm3d.html#dokumente)). Both layers represent areas used for specific activities. Freizeitareal identifies areas used for recreational purposes and Nutzungsareal represents areas such as hospitals, cemeteries, historical sites or incineration plants. As a ratio of the available dry-land in a hex, these features are relatively small (less than 10%) of the total dry-land. For identified features within a bounding hex the magnitude in meters² of these variables is scaled between 0 and 1, thus the scaled value represents the size of the feature in relation to all other measured values for that feature from all other hexagons.\n",
    "\n",
    "1. Recreation: parks, sports fields, attractions\n",
    "2. Infrastructure: Schools, Hospitals, cemeteries, powerplants\n",
    "\n",
    "__Streets and roads__\n",
    "\n",
    "Streets and roads are the labled polylines from the _TLM Strasse_ map layer defined in ([swissTLMRegio product information](https://www.swisstopo.admin.ch/fr/geodata/landscape/tlm3d.html#dokumente)). All polyines from the map layer within a bounding hex are merged (disolved in QGIS commands) and the combined length of the polylines, in meters, is the magnitude of the variable for the bounding hex.\n",
    "\n",
    "__Covariance of land-use variables__\n",
    "\n",
    "The training data and the testing data come from the same lake. The locations surveyed in 2022 have different coefficients than 2021. Note how different the covaraince with cities is to woods and undefined land."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "def sampler_from_multinomial(normed, xrange, nsamples):\n",
    "    \n",
    "    choose = np.random.default_rng()\n",
    "    nunique = np.unique(normed)\n",
    "    norm_nunique = nunique/np.sum(nunique)\n",
    "    found = choose.multinomial(1, pvals=norm_nunique, size=nsamples)\n",
    "    ft = found.sum(axis=0)\n",
    "    samples = []\n",
    "    for i, asum in enumerate(ft):\n",
    "        if asum == 0:\n",
    "            samples += [0]\n",
    "        else:\n",
    "            choices = np.where(normed == nunique[i])\n",
    "            samps = choose.choice(choices[0], size=asum)\n",
    "            samples.extend(xrange[samps])\n",
    "\n",
    "    return samples, nunique, norm_nunique, ft\n",
    "\n",
    "def period_pieces(start, end, data):\n",
    "    # the results in pieces per meter for one code from a subset of data\n",
    "    date_mask = (data[\"date\"] >= start) & (data[\"date\"] <= end)\n",
    "    period_one = data[date_mask]\n",
    "    pone_pcs = period_one.pcs_m.values\n",
    "\n",
    "    return pone_pcs\n",
    "\n",
    "def period_k_and_n(data, xrange, add_one=False):\n",
    "\n",
    "    pone_k = [(data >= x).sum() for x in xrange]\n",
    "    pone_notk = [(data < x).sum() for x in xrange]\n",
    "\n",
    "    if add_one:\n",
    "        # if the use is for beta dist. This is the same\n",
    "        # as mulitplying the likelihood * uninform prior (0.5) or beta(1,1)\n",
    "        pone_k_n_minus_k = [(x+1, len(data) - x+1) for x in pone_k]\n",
    "    else:\n",
    "        pone_k_n_minus_k = [(x, len(data) - x) for x in pone_k]\n",
    "        \n",
    "    \n",
    "\n",
    "    return np.array(pone_k), np.array(pone_notk), np.array(pone_k_n_minus_k)\n",
    "\n",
    "def period_beta(k):\n",
    "    \n",
    "         \n",
    "    return beta(*k)\n",
    "        \n",
    "\n",
    "def current_possible_prior_locations(landuse, locations, attribute):    \n",
    "\n",
    "    # indentify the magnitude(s) of the attribute of interest from the\n",
    "    # locations in the current data there may be more than one, in this \n",
    "    # example we use all the possible magnitudes for the attribute\n",
    "    # locations = data[data.city == city].location.unique()\n",
    "\n",
    "    # magnitudes for the attribute from all the locations in the municipality\n",
    "    moa = magnitude_of_attribute = landuse.loc[locations][attribute].unique().astype('int')\n",
    "\n",
    "    # identify locations that have the same attribute by magnitude of attribute\n",
    "    possible_locations = landuse[landuse[attribute].isin(moa)].index\n",
    "\n",
    "    # remove the locations that are in the likelihood function\n",
    "    prior_locations = [x for x in possible_locations if x not in locations]\n",
    "\n",
    "    return locations, possible_locations, prior_locations\n",
    "\n",
    "\n",
    "def make_expected(lh_tuple, prior_tuple, xrange):\n",
    "    res = []\n",
    "    betas=[]\n",
    "    # print(lh_tuple, prior_tuple)\n",
    "    for i in np.arange(len(xrange)):\n",
    "        alpha = prior_tuple[i][0]\n",
    "        betai = prior_tuple[i][1]\n",
    "        success = lh_tuple[i][0]\n",
    "        n = lh_tuple[i][1] + lh_tuple[i][0] \n",
    "        numerator = alpha + success\n",
    "        denominator = alpha + betai + n\n",
    "        if numerator == 0:\n",
    "            numerator = 1\n",
    "        abeta = beta(numerator, (betai + lh_tuple[i][1] + lh_tuple[i][0])).mean()\n",
    "        betas.append(abeta)\n",
    "        # print(alpha, betai, success, numerator, n, denominator)\n",
    "        if numerator >= denominator:\n",
    "            numerator = denominator-1\n",
    "            \n",
    "        expected = numerator/denominator\n",
    "        res.append(expected)\n",
    "    return np.array(res), np.array(betas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "comb_lu_agg = pd.read_csv(\"resources/data/u_comb_lu_cover_street_rivers.csv\")\n",
    "\n",
    "lu_scaled = comb_lu_agg.pivot(columns=\"use\", values=\"scaled\", index=\"slug\").fillna(0)\n",
    "\n",
    "lu_magnitude = comb_lu_agg.pivot(columns=\"use\", values=\"magnitude\", index=\"slug\").fillna(0)\n",
    "\n",
    "lu_binned = comb_lu_agg.pivot(columns=\"use\", values=\"binned\", index=\"slug\").fillna(0)\n",
    "\n",
    "# not_these = ['amphion', 'anthy', 'excenevex', 'lugrin', 'meillerie', 'saint-disdille', 'tougues']\n",
    "merge_locations = cbd.slug.unique()\n",
    "cbdu = cbd[~cbd.slug.isin(not_these)].merge(lu_scaled[lu_scaled.index.isin(merge_locations )], left_on=\"slug\", right_index=True, validate=\"many_to_one\", how=\"outer\")\n",
    "\n",
    "cbdu[\"use group\"] = cbdu.code.map(lambda x: use_groups_i[x])\n",
    "\n",
    "cbdu[\"ug\"] = cbdu[\"use group\"].apply(lambda x: abbrev_use_g[x])\n",
    "cbdu[cbdu[\"use group\"] == 'Personal consumption'].code.unique()\n",
    "cbdu[\"date\"] = pd.to_datetime(cbdu[\"date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "attribute_columns = [x for x in lu_scaled.columns if x not in [\"Geroell\", \"Stausee\", \"See\", \"Sumpf\", \"Stadtzentr\", \"Fels\"]]\n",
    "cbdu.rename(columns={\"pcs/m\":\"pcs_m\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# !!Bug these locations not found in the \n",
    "# land use index \"['cully-p', 'preverenges-p', 'tolochenaz-p', 'vidy-p']\n",
    "# raises exception with: \n",
    "# trc = lu_scaled.loc[tr_locs][attribute_columns]\n",
    "\n",
    "# tst_locs = cbdu[(cbdu.Project == 'Testing')].slug.unique()\n",
    "# tr_locs = cbdu[(cbdu.Project == 'Training')].slug.unique()\n",
    "\n",
    "\n",
    "# english_column_names = {\n",
    "#     \"Obstanlage\":\"Orchards\",\n",
    "#     \"Reben\": \"Vineyards\",\n",
    "#     \"Siedl\": \"Buildings\",\n",
    "#     \"Strasse\": \"Streets\",\n",
    "#     \"Wald\": \"Woods\",\n",
    "#     \"infrastructure\":\"Infrastructure\",\n",
    "#     \"recreation\":\"Recreation\",\n",
    "#     \"undefined\":\"Undefined\"\n",
    "# }\n",
    "\n",
    "# trc = lu_scaled.loc[tr_locs][attribute_columns]\n",
    "# tst = lu_scaled.loc[tst_locs][attribute_columns]\n",
    "\n",
    "# trc.rename(columns=english_column_names, inplace=True)\n",
    "# tst.rename(columns=english_column_names, inplace=True)\n",
    "\n",
    "# corr_tst = tst.corr()\n",
    "# corr_trc = trc.corr()\n",
    "\n",
    "# mask_tr = np.triu(np.ones_like(corr_trc, dtype=bool))\n",
    "# mask_ts = np.triu(np.ones_like(corr_tst, dtype=bool))\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# sns.heatmap(corr_trc, mask=mask_tr, cmap=\"YlOrBr\", ax=ax)\n",
    "\n",
    "\n",
    "# ax.set_ylabel(\"\")\n",
    "# ax.set_xlabel(\"\")\n",
    "# ax.set_title(\"Training data\", loc=\"left\")\n",
    "# plt.tight_layout()\n",
    "\n",
    "# glue(\"corr_training\", fig, display=False)\n",
    "# plt.close()\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# sns.heatmap(corr_tst, mask=mask_ts, cmap=\"YlOrBr\", ax=ax)\n",
    "\n",
    "# ax.set_ylabel(\"\")\n",
    "# ax.set_xlabel(\"\")\n",
    "# ax.set_title(\"Testing data\", loc=\"left\")\n",
    "# plt.tight_layout()\n",
    "\n",
    "# glue(\"corr_testing\", fig, display=False)\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Results Lake Geneva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- > 1. Given the data from 2022, Is there an increase, decrease or no change in the expected survey results given the consolidated results from 2015 - 2021?\n",
    "\n",
    "The __combined daily total__ is the sum of the objects of interest per sample. In this case we are concerned with the objects listed in the [Objects of interest](objects-of-interest) section. The average number of objects counted and indentified per meter is expected to decline going into 2024. However, the reduction is minimal (Figure 5) and not equally spread between all object groups or survey locations. Which means that these changes will not be readily observable and also the sampling distribution for 2024 will be very close to 2021 (Table 4, Figure 6). -->\n",
    "\n",
    "```{admonition} Given the data from 2022, Is there an increase, decrease or no change in the expected survey results given the consolidated results from 2015 - 2021?\n",
    "\n",
    "The average number of objects counted and indentified per meter is expected to decline going into 2024. However, the reduction is minimal (Figure 5) and not equally spread between all object groups or survey locations. Which means that these changes will not be readily observable and also the sampling distribution for 2024 will be very close to 2021 (Table 4, Figure 6). The __combined daily total__ is the sum of the objects of interest per sample. In this case we are concerned with the objects listed in the [Objects of interest](objects-of-interest) section. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# not_these = ['amphion', 'anthy', 'excenevex', 'lugrin', 'meillerie', 'saint-disdille', 'tougues']\n",
    "g_resa = cbdu[~cbdu.slug.isin(not_these)].copy()\n",
    "g_res = g_resa.groupby(['loc_date', 'date','slug', 'city', 'Project', 'ug'], as_index=False).agg({'pcs_m':'sum', 'quantity':'sum'})\n",
    "g_res.rename(columns={\"ug\":\"code\"}, inplace=True)\n",
    "index_range = (0.0, 10)\n",
    "\n",
    "xrange =  np.arange(*index_range, step=.01)\n",
    "\n",
    "# define the uninformed prior as either a float\n",
    "# or coefficients for the beta dist\n",
    "uninformed_prior = np.array([0.5 for x in xrange])\n",
    "uninformed_tuple = np.array([(1,1) for x in xrange])\n",
    "\n",
    "# the data is aggregated on loc_date for both sets\n",
    "train_dt = g_resa[g_resa.Project == \"Training\"].groupby('loc_date', as_index=False).agg({\"pcs_m\":\"sum\", \"quantity\":\"sum\"})\n",
    "test_dt = g_resa[g_resa.Project == \"Testing\"].groupby('loc_date', as_index=False).agg({\"pcs_m\":\"sum\", \"quantity\":\"sum\"})\n",
    "\n",
    "# get the values or arrays of interest\n",
    "train_dt_vals = train_dt.pcs_m.values\n",
    "test_dt_vals = test_dt.pcs_m.values\n",
    "\n",
    "# get k, and n-minus k\n",
    "t_dt, t_dt_notk, t_dt_n_minus_k = period_k_and_n(train_dt_vals, xrange)\n",
    "\n",
    "# make the likelihood parameters\n",
    "lhx = list(zip(t_dt, t_dt_notk))\n",
    "\n",
    "# make the prior distribution\n",
    "p_ui, p_beta = make_expected(lhx, uninformed_tuple, xrange)\n",
    "\n",
    "# the uninformed beta approximation of the prior data\n",
    "prior_beta = [period_beta(x) for x in t_dt_n_minus_k]\n",
    "prior_bmean = [x.mean() for x in prior_beta]\n",
    "\n",
    "test_dt, test_dt_notk, test_dt_n_minus_k = period_k_and_n(test_dt_vals, xrange)\n",
    "\n",
    "# make the likelihood parameters\n",
    "lhx = list(zip(test_dt, test_dt_notk))\n",
    "\n",
    "# make the prior distribution\n",
    "test_ui, t_beta = make_expected(lhx, uninformed_tuple, xrange)\n",
    "\n",
    "# the uninformed beta approximation of the prior data\n",
    "test_beta = [period_beta(x) for x in test_dt_n_minus_k]\n",
    "test_bmean = [x.mean() for x in test_beta]\n",
    "\n",
    "# posterior\n",
    "aprior = test_dt_n_minus_k + t_dt_n_minus_k\n",
    "outlook_2024, _ = make_expected(test_dt_n_minus_k , t_dt_n_minus_k, xrange)\n",
    "results=pd.DataFrame({\"x\":xrange, \"p\":p_ui})\n",
    "results[\"pn\"] = results.p/results.p.sum()\n",
    "\n",
    "results[\"Informed\"] = outlook_2024\n",
    "results[\"Ip_n\"] = results.Informed/results.Informed.sum()\n",
    "results[\"Uninformed post\"] = test_ui\n",
    "\n",
    "# the quantiles from the observed data\n",
    "prior_quants = np.quantile(train_dt_vals, some_quants)\n",
    "post_quants = np.quantile(test_dt_vals, some_quants)\n",
    "\n",
    "Ip_n = outlook_2024/np.sum(outlook_2024)\n",
    "\n",
    "# samples from posterior \n",
    "choose = np.random.default_rng()\n",
    "sim_2024 = choose.choice(xrange, 1000, p=Ip_n)\n",
    "median_2024 = np.median(sim_2024)\n",
    "\n",
    "# observed in relation to predicted\n",
    "ppf = predicted_summary(test_dt_vals, train_dt_vals, prior_quants, median_2024)\n",
    "caption=\"Previous and expected median values of the combined daily totals of the Unkown objects group\"\n",
    "\n",
    "ppf_d = ppf.style.format(precision=2).set_caption(caption).set_table_styles(table_large_font)\n",
    "glue(\"comb-2024-meds\", ppf_d, display=False)\n",
    "\n",
    "# comparing training to testing\n",
    "unks_sum_table = training_testing_compare(test_dt_vals, train_dt_vals, post_quants, prior_quants)\n",
    "caption = \"The observed values from the training and testing data.\"\n",
    "sum_table = unks_sum_table.set_caption(caption)\n",
    "glue(\"comb-summary\", sum_table, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# points\n",
    "median_prior = np.median(train_dt_vals)\n",
    "median_lh = np.median(test_dt_vals)\n",
    "median_post = results[results[\"Informed\"].between(.49, .51)][\"x\"].values[0]\n",
    "quants_2024 = np.quantile(sim_2024, some_quants)\n",
    "\n",
    "# posterior\n",
    "ax.plot(xrange, outlook_2024, c=\"magenta\", linewidth=4,linestyle=':', zorder=20, label='Outlook 2024')\n",
    "ax.plot([median_post], [.5],  c=\"black\", markersize=6, marker=\"x\", zorder=27, label=\"Expected median 2024\")\n",
    "ax.plot(xrange, test_ui, c=\"darkslategrey\",  linestyle=':', linewidth=3, zorder=11, alpha=0.5,  label='2022')\n",
    "ax.plot([median_lh], [.5], c=\"blue\", markersize=5, marker=\"o\", zorder=25, label=\"2022 median\")\n",
    "ax.plot(xrange, p_ui, c=\"cornflowerblue\", linestyle=':',  linewidth=4, zorder=11, alpha=0.5, label=\"2021\")\n",
    "ax.plot([median_prior], [.5], c=\"red\", markersize=5, marker=\"o\", zorder=25, label=\"2021 median\")\n",
    "\n",
    "# 50% IQR\n",
    "ax.axvspan(xmin=prior_quants[1], xmax=prior_quants[5], ymin=0.25, ymax=0.75, facecolor='cornflowerblue',  edgecolor='cornflowerblue', zorder=13, alpha=0.2, label=\"IQR - 2021\")\n",
    "ax.axvspan(xmin=post_quants[1], xmax=post_quants[5], ymin=0.25, ymax=0.75, facecolor='darkslategrey', edgecolor='darkslategrey', linestyle=\"-.\", zorder=13, alpha=0.2, label=\"IQR - 2022\")\n",
    "ax.axvspan(xmin=quants_2024[1], xmax=quants_2024[5], ymin=0.25, ymax=0.75, facecolor='black',  edgecolor='black',  zorder=13, alpha=0.2, label=\"IQR - 2024\")\n",
    "\n",
    "ax.set_xlabel('pcs/m')\n",
    "ax.set_xlim(-.1, 10)\n",
    "ax.set_ylabel('probability')\n",
    "h, l = ax.get_legend_handles_labels()\n",
    "ax.legend(h, l, bbox_to_anchor=(0,1.05), loc=\"lower left\", ncol=2 )\n",
    "glue('comb-outlook-2024', fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Previous and expected survey totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "unin_2022 = results[\"Uninformed post\"].values\n",
    "\n",
    "sns.ecdfplot([*train_dt_vals, *test_dt_vals], label=\"Observed 2015 - 2022\",  c=\"black\", stat=\"proportion\", ax=ax, zorder=10)\n",
    "sns.ecdfplot(train_dt_vals, ax=ax, label=\"Observed 2015 - 2021\",color=\"cornflowerblue\", stat=\"proportion\", zorder=10)\n",
    "sns.ecdfplot(test_dt_vals, ax=ax, label=\"Observed 2022\", color=\"darkslategrey\", stat=\"proportion\", zorder=10)\n",
    "\n",
    "ax.plot(xrange, (1-outlook_2024), linestyle=':', c=\"magenta\", linewidth=3,label=\"Expected 2024\", zorder=11)\n",
    "\n",
    "sns.histplot(train_dt_vals, ax=ax, label=\"Observed 2021\", color=\"cornflowerblue\", alpha=0.5, zorder=0, stat=\"probability\")\n",
    "sns.histplot(sim_2024, ax=ax, label=\"Expected 2024\", color=\"black\", zorder=2, alpha=0.5, edgecolor=\"magenta\", linewidth=1.2, stat=\"probability\")\n",
    "sns.histplot(test_dt_vals, ax=ax, label=\"Observed 2022\", color=\"darkslategrey\", alpha=0.5, stat=\"probability\", zorder=0)\n",
    "\n",
    "ax.set_xlabel('pcs/m')\n",
    "ax.set_xlim(-.1, 10)\n",
    "ax.set_ylabel('probability')\n",
    "\n",
    "ax.legend(bbox_to_anchor=(0,1.05), loc=\"lower left\", ncol=2 )\n",
    "glue('comb-predicted_samples', fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "|     Figure 5, Table 3    |     Table 4, Figure 6        | \n",
    "|:------------------------:|:----------------------------:|\n",
    "|{glue:}`comb-outlook-2024` | {glue:}`comb-2024-meds`|\n",
    "|{glue:}`comb-summary` | {glue:}`comb-predicted_samples`|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# select the code, city and attribute\n",
    "\n",
    "code_index = 0\n",
    "city_index = 0\n",
    "attribute_index = 2\n",
    "\n",
    "this_code =  code_groups[code_index]\n",
    "this_attribute = attribute_columns[attribute_index]\n",
    "start, end = \"2015-11-15\", \"2021-05-31\"\n",
    "\n",
    "# define the prior, likelihood data and likelihood locations\n",
    "prior_data = g_res[(g_res.Project == 'Training')&(g_res.code == this_code)]\n",
    "lh_data = g_res[(g_res.Project == 'Testing')&(g_res.code == this_code)]\n",
    "lh_locations = lh_data.slug.unique()\n",
    "\n",
    "regions = lac_leman_regions[~lac_leman_regions.slug.isin(not_these)].copy()\n",
    "lh_regions = regions[regions.slug.isin(lh_locations)].alabel.unique()\n",
    "regional_locations = regions[regions.alabel.isin(lh_regions)].slug.unique()\n",
    "land_use_data_of_interest = lu_binned.loc[regional_locations]\n",
    "\n",
    "locations, possible_locations, prior_locations = current_possible_prior_locations(land_use_data_of_interest, regional_locations, this_attribute)\n",
    "\n",
    "prior_args = {\n",
    "    'prior_data':prior_data[prior_data.slug.isin(regional_locations)],\n",
    "    'start': start,\n",
    "    'end': end,\n",
    "    'xrange':xrange,\n",
    "    'uninformed_prior': uninformed_tuple,\n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "# grid approximation of the prior\n",
    "grid_prior, beta_prior, prior_k_n, prior_df, pcs = prior_distributions(**prior_args)\n",
    "\n",
    "posterior_args = {\n",
    "    'lh_data':lh_data,\n",
    "    'start': \"2022-01-01\",\n",
    "    'end': \"2022-12-31\",\n",
    "    'un_informed': uninformed_tuple,\n",
    "    'code':this_code,\n",
    "    'informed_prior': prior_k_n\n",
    "}\n",
    "\n",
    "# grid approximation of posterior\n",
    "informed, uninformed, beta_p, lh_pcs = posterior_distribution(**posterior_args)\n",
    "\n",
    "# the quantiles from the observed data\n",
    "prior_quants = np.quantile(pcs, some_quants)\n",
    "post_quants = np.quantile(lh_pcs, some_quants)\n",
    "\n",
    "# data frame with normalized results\n",
    "post_df = make_results_df(prior_df.copy(), informed, source=\"Informed post\", source_norm=\"Ip_n\")\n",
    "post_df = make_results_df(post_df, uninformed, source=\"Uninformed post\", source_norm=\"Un_n\")\n",
    "\n",
    "# samples from posterior \n",
    "choose = np.random.default_rng()\n",
    "sim_2024 = choose.choice(xrange, len(lh_pcs), p=post_df[\"Ip_n\"].values)\n",
    "median_2024 = post_df.loc[post_df[\"Informed post\"].between(.49, .51)].index.values.mean()/100\n",
    "\n",
    "summaries = []\n",
    "dfs = []\n",
    "# observed in relation to predicted\n",
    "ppf = predicted_summary(lh_pcs, pcs, prior_quants, median_2024)\n",
    "summaries.append(ppf)\n",
    "dfs.append(post_df.rename(columns={\"Informed post\":\"2024\", \"x\":\"pcs/m\"}))\n",
    "caption=\"Previous and expected median values of the combined daily totals of the Unkown objects group\"\n",
    "\n",
    "ppf_d = ppf.style.format(precision=2).set_caption(caption).set_table_styles(table_large_font)\n",
    "glue(\"unk-2024-meds\", ppf_d, display=False)\n",
    "\n",
    "# comparing training to testing\n",
    "unks_sum_table = training_testing_compare(lh_pcs, pcs, post_quants, prior_quants)\n",
    "caption = \"The observed values from the training and testing data.\"\n",
    "sum_table = unks_sum_table.set_caption(caption)\n",
    "glue(\"unk-summary\", sum_table, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# bayes rules in python\n",
    "# apie = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "# alength = np.linspace(start=0, stop=1, num=6)\n",
    "# prior_tuples = [(2,2) for _ in alength]\n",
    "# prior = [beta.pdf(alength[i], *x).mean() for i, x in enumerate(prior_tuples)]\n",
    "# likelihood = [binom.pmf(9, 10, x) for x in alength]\n",
    "# print(np.round(prior, 2))\n",
    "# print(np.round(likelihood, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "unin_2022 = post_df[\"Uninformed post\"].values\n",
    "\n",
    "sns.ecdfplot([*pcs, *lh_pcs], label=\"Observed 2015 - 2022\",  c=\"black\", stat=\"proportion\", ax=ax, zorder=10)\n",
    "sns.ecdfplot(pcs, ax=ax, label=\"Observed 2015 - 2021\",color=\"cornflowerblue\", stat=\"proportion\", zorder=10)\n",
    "sns.ecdfplot(lh_pcs, ax=ax, label=\"Observed 2022\", color=\"darkslategrey\", stat=\"proportion\", zorder=10)\n",
    "\n",
    "ax.plot(xrange, (1-informed), linestyle=':', c=\"magenta\", linewidth=3,label=\"Expected 2024\", zorder=11)\n",
    "\n",
    "sns.histplot(pcs, ax=ax, label=\"Observed 2021\", color=\"cornflowerblue\", alpha=0.5, zorder=0, stat=\"probability\")\n",
    "sns.histplot(sim_2024, ax=ax, label=\"Expected 2024\", color=\"black\", zorder=2, alpha=0.5, edgecolor=\"magenta\", linewidth=1.2, stat=\"probability\")\n",
    "sns.histplot(lh_pcs, ax=ax, label=\"Observed 2022\", color=\"darkslategrey\", alpha=0.5, stat=\"probability\", zorder=0)\n",
    "\n",
    "ax.set_xlabel('pcs/m')\n",
    "ax.set_xlim(-.1, 3)\n",
    "ax.set_ylabel('probability')\n",
    "# h, l = ax.get_legend_handles_labels()\n",
    "ax.legend(bbox_to_anchor=(0,1.05), loc=\"lower left\", ncol=2 )\n",
    "glue('gfrags-predicted_samples', fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# points\n",
    "median_prior = np.median(pcs)\n",
    "median_lh = np.median(lh_pcs)\n",
    "median_post = post_df[post_df[\"Informed post\"].between(.49, .51)][\"x\"].values[0]\n",
    "quants_2024 = np.quantile(sim_2024, some_quants)\n",
    "\n",
    "# posterior\n",
    "ax.plot(xrange, informed, c=\"magenta\", linewidth=3,linestyle=':', zorder=20, label='Outlook 2024')\n",
    "ax.plot([median_post], [.5],  c=\"black\", markersize=6, marker=\"x\", zorder=27, label=\"Expected median 2024\")\n",
    "ax.plot(xrange, uninformed, c=\"darkslategrey\",  linestyle=':', linewidth=3, zorder=11, alpha=0.5,  label='2022')\n",
    "ax.plot([median_lh], [.5], c=\"blue\", markersize=5, marker=\"o\", zorder=25, label=\"2022 median\")\n",
    "ax.plot(xrange, grid_prior,  c=\"cornflowerblue\", linestyle=':',  linewidth=4, zorder=11, alpha=0.5, label=\"2021\")\n",
    "ax.plot([median_prior], [.5], c=\"red\", markersize=5, marker=\"o\", zorder=25, label=\"2021 median\")\n",
    "\n",
    "# 50% IQR\n",
    "ax.axvspan(xmin=prior_quants[1], xmax=prior_quants[5], ymin=0.25, ymax=0.75, facecolor='cornflowerblue', zorder=13, alpha=0.2, label=\"IQR - 2021\")\n",
    "ax.axvspan(xmin=post_quants[1], xmax=post_quants[5], ymin=0.25, ymax=0.75, facecolor='darkslategrey', zorder=13, alpha=0.2, label=\"IQR - 2022\")\n",
    "ax.axvspan(xmin=quants_2024[1], xmax=quants_2024[5], ymin=0.25, ymax=0.75, facecolor='black', zorder=13, alpha=0.2, label=\"IQR - 2024\")\n",
    "\n",
    "ax.set_xlabel('pcs/m')\n",
    "ax.set_xlim(-.1, 3)\n",
    "ax.set_ylabel('probability')\n",
    "h, l = ax.get_legend_handles_labels()\n",
    "ax.legend(h[:-2], l[:-2], bbox_to_anchor=(0,1.05), loc=\"lower left\", ncol=2 )\n",
    "glue('gfrags-outlook-2024', fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "code_index = 1\n",
    "city_index = 0\n",
    "attribute_index = 2\n",
    "\n",
    "this_code =  code_groups[code_index]\n",
    "this_attribute = attribute_columns[attribute_index]\n",
    "start, end = \"2015-11-15\", \"2021-05-31\"\n",
    "# some_quants = [.03, .25, .48, .5, .52, .75, .97]\n",
    "\n",
    "# define the prior, likelihood data and likelihood locations\n",
    "prior_data = g_res[(g_res.Project == 'Training')&(g_res.code == this_code)]\n",
    "lh_data = g_res[(g_res.Project == 'Testing')&(g_res.code == this_code)]\n",
    "lh_locations = lh_data.slug.unique()\n",
    "\n",
    "regions = lac_leman_regions[~lac_leman_regions.slug.isin(not_these)].copy()\n",
    "lh_regions = regions[regions.slug.isin(lh_locations)].alabel.unique()\n",
    "regional_locations = regions[regions.alabel.isin(lh_regions)].slug.unique()\n",
    "land_use_data_of_interest = lu_binned.loc[regional_locations]\n",
    "\n",
    "locations, possible_locations, prior_locations = current_possible_prior_locations(land_use_data_of_interest, regional_locations, this_attribute)\n",
    "\n",
    "# grid approximation of the prior\n",
    "prior_args = {\n",
    "    'prior_data':prior_data[prior_data.slug.isin(regional_locations)],\n",
    "    'start': start,\n",
    "    'end': end,\n",
    "    'xrange':xrange,\n",
    "    'uninformed_prior': uninformed_tuple,\n",
    "}\n",
    "\n",
    "grid_prior, beta_prior, prior_k_n, prior_df, pcs = prior_distributions(**prior_args)\n",
    "\n",
    "posterior_args = {\n",
    "    'lh_data':lh_data,\n",
    "    'start': \"2022-01-01\",\n",
    "    'end': \"2022-12-31\",\n",
    "    'un_informed': uninformed_tuple,\n",
    "    'code':this_code,\n",
    "    'informed_prior': prior_k_n\n",
    "}\n",
    "\n",
    "# grid approximation of posterior\n",
    "informed, uninformed, beta_p, lh_pcs = posterior_distribution(**posterior_args)\n",
    "\n",
    "# the quantiles from the observed data\n",
    "prior_quants = np.quantile(pcs, some_quants)\n",
    "post_quants = np.quantile(lh_pcs, some_quants)\n",
    "\n",
    "# data frame with normalized results\n",
    "post_df = make_results_df(prior_df.copy(), informed, source=\"Informed post\", source_norm=\"Ip_n\")\n",
    "post_df = make_results_df(post_df, uninformed, source=\"Uninformed post\", source_norm=\"Un_n\")\n",
    "\n",
    "# samples from posterior \n",
    "choose = np.random.default_rng()\n",
    "sim_2024 = choose.choice(xrange, 1000, p=post_df[\"Ip_n\"].values)\n",
    "median_2024 = post_df.loc[post_df[\"Informed post\"].between(.49, .51)].index.values.mean()/100\n",
    "\n",
    "# observed in relation to predicted\n",
    "ppfi = predicted_summary(lh_pcs, pcs, prior_quants, median_2024 )\n",
    "caption=\"Previous and expected median values of indentified Personal consumption group\"\n",
    "\n",
    "ppf_di = ppfi.style.format(precision=2).set_caption(caption).set_table_styles(table_large_font)\n",
    "glue(\"pc-2024-meds\", ppf_di, display=False)\n",
    "\n",
    "# comparing training to testing\n",
    "pc_sum_table = training_testing_compare(lh_pcs, pcs, post_quants, prior_quants)\n",
    "caption = \"The observed values from the training and testing data.\"\n",
    "pc_table = pc_sum_table.set_caption(caption)\n",
    "glue(\"pc-summary\", pc_table, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "unin_2022 = post_df[\"Uninformed post\"].values\n",
    "\n",
    "# hists = pd.DataFrame({\"Observed 2021\": pcs, \"Expected 2024\": sim_2024, \"Observed 2022\": lh_pcs}, index=xrange)\n",
    "\n",
    "sns.ecdfplot([*pcs, *lh_pcs], label=\"Observed 2015 - 2022\",  c=\"black\", stat=\"proportion\", ax=ax, zorder=10)\n",
    "sns.ecdfplot(pcs, ax=ax, label=\"Observed 2015 - 2021\",color=\"cornflowerblue\", stat=\"proportion\", zorder=10)\n",
    "sns.ecdfplot(lh_pcs, ax=ax, label=\"Observed 2022\", color=\"darkslategrey\", stat=\"proportion\", zorder=10)\n",
    "\n",
    "ax.plot(xrange, (1-informed), linestyle=':', c=\"magenta\", linewidth=3,label=\"Expected 2024\", zorder=11)\n",
    "\n",
    "sns.histplot(pcs, ax=ax, label=\"Observed 2021\", color=\"cornflowerblue\", alpha=0.5, zorder=0, stat=\"probability\")\n",
    "sns.histplot(sim_2024, ax=ax, label=\"Expected 2024\", color=\"black\", zorder=2, alpha=0.5, edgecolor=\"magenta\", linewidth=1, stat=\"probability\")\n",
    "sns.histplot(lh_pcs, ax=ax, label=\"Observed 2022\", color=\"darkslategrey\", alpha=0.5, stat=\"probability\", zorder=0)\n",
    "\n",
    "ax.set_xlabel('pcs/m')\n",
    "ax.set_xlim(-.1, 3)\n",
    "ax.set_ylabel('probability')\n",
    "# h, l = ax.get_legend_handles_labels()\n",
    "ax.legend(bbox_to_anchor=(0,1.05), loc=\"lower left\", ncol=2 )\n",
    "glue('pc-predicted_samples', fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# points\n",
    "median_prior = np.median(pcs)\n",
    "median_lh = np.median(lh_pcs)\n",
    "median_post = post_df[post_df[\"Informed post\"].between(.49, .51)][\"x\"].values[0]\n",
    "quants_2024 = np.quantile(sim_2024, some_quants)\n",
    "\n",
    "# posterior\n",
    "ax.plot(xrange, informed, c=\"magenta\", linewidth=3,linestyle=':', zorder=20, label='Outlook 2024')\n",
    "ax.plot([median_post], [.5],  c=\"black\", markersize=6, marker=\"x\", zorder=27, label=\"Expected median 2024\")\n",
    "ax.plot(xrange, uninformed, c=\"darkslategrey\",  linestyle=':', linewidth=3, zorder=11, alpha=0.5,  label='2022')\n",
    "ax.plot([median_lh], [.5], c=\"blue\", markersize=5, marker=\"o\", zorder=25, label=\"2022 median\")\n",
    "ax.plot(xrange, grid_prior, c=\"cornflowerblue\", linestyle=':',  linewidth=4, zorder=11, alpha=0.5, label=\"2021\")\n",
    "ax.plot([median_prior], [.5], c=\"red\", markersize=5, marker=\"o\", zorder=25, label=\"2021 median\")\n",
    "\n",
    "# 50% IQR\n",
    "ax.axvspan(xmin=prior_quants[1], xmax=prior_quants[5], ymin=0.25, ymax=0.75, facecolor='cornflowerblue', zorder=13, alpha=0.2, label=\"IQR - 2021\")\n",
    "ax.axvspan(xmin=post_quants[1], xmax=post_quants[5], ymin=0.25, ymax=0.75, facecolor='darkslategrey', zorder=13, alpha=0.2, label=\"IQR - 2022\")\n",
    "ax.axvspan(xmin=quants_2024[1], xmax=quants_2024[5], ymin=0.25, ymax=0.75, facecolor='black', zorder=13, alpha=0.2, label=\"IQR - 2024\")\n",
    "ax.plot(xrange, beta_prior, c=\"lightsteelblue\",  linewidth=3, zorder=10, alpha=0.5, label=\"Outlook-beta 2022\")\n",
    "ax.plot(xrange, beta_p, c=\"lightsteelblue\",  linewidth=3, zorder=10, alpha=0.5, label=\"Outlook-beta 2022\")\n",
    "\n",
    "ax.set_xlabel('pcs/m')\n",
    "ax.set_xlim(-.1, 3)\n",
    "ax.set_ylabel('probability')\n",
    "h, l = ax.get_legend_handles_labels()\n",
    "ax.legend(h[:-2], l[:-2], bbox_to_anchor=(0,1.05), loc=\"lower left\", ncol=2 )\n",
    "glue('pc-outlook-2024', fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- |                        |                               |\n",
    "|------------------------|-------------------------------|\n",
    "|{glue:}`personal-consumption-2024`| {glue:}`pc-2024-meds`| \n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# select the code, city and attribute\n",
    "\n",
    "code_index = 1\n",
    "city_index = 0\n",
    "attribute_index = 2\n",
    "\n",
    "\n",
    "this_code =  code_groups[code_index]\n",
    "\n",
    "attribute_index = 2\n",
    "this_attribute = attribute_columns[attribute_index]\n",
    "start, end = \"2015-11-15\", \"2021-05-31\"\n",
    "\n",
    "prior_data = g_res[(g_res.Project == 'Training')&(g_res.code == this_code)]\n",
    "lh_data = g_res[(g_res.Project == 'Testing')&(g_res.code == this_code)]\n",
    "lh_locations = lh_data.slug.unique()\n",
    "\n",
    "lh_vals = lh_data[\"pcs_m\"].values\n",
    "post_quants = np.quantile(lh_vals, some_quants)\n",
    "prior_vals = g_res[g_res.code == this_code][\"pcs_m\"].values\n",
    "prior_quants = np.quantile(prior_vals, [.03, .25, .48, .5, .52, .75, .97])\n",
    "\n",
    "prior_args = {\n",
    "    'prior_data':prior_data,\n",
    "    'start': start,\n",
    "    'end': end,\n",
    "    'xrange':xrange,\n",
    "    'uninformed_prior': uninformed_tuple,\n",
    "   \n",
    "}\n",
    "\n",
    "grid_prior, beta_prior, prior_k_n, prior_df, pcs = prior_distributions(**prior_args)\n",
    "\n",
    "posterior_args = {\n",
    "    'lh_data':lh_data,\n",
    "    'start': \"2022-01-01\",\n",
    "    'end': \"2022-12-31\",\n",
    "    'un_informed': uninformed_tuple,\n",
    "    'informed_prior': prior_k_n\n",
    "}\n",
    "\n",
    "\n",
    "informed, uninformed, beta_p, lh_pcs = posterior_distribution(**posterior_args)\n",
    "post_df = make_results_df(prior_df, informed, source=\"Informed post\", source_norm=\"Ip_n\")\n",
    "post_df = make_results_df(post_df, uninformed, source=\"Uninformed post\", source_norm=\"Un_n\")\n",
    "\n",
    "results_df = pd.DataFrame({\"pcs/m\":xrange, \"2024\":informed})\n",
    "results_df[\"2024 pmf\"] = results_df[\"2024\"]/results_df[\"2024\"].sum()\n",
    "median_2024 = results_df.loc[results_df[\"2024\"].between(.49, .51), \"pcs/m\"].mean()\n",
    "\n",
    "ppf = predicted_summary(lh_pcs, pcs, prior_quants, median_2024)\n",
    "summaries.append(ppf)\n",
    "dfs.append(results_df)\n",
    "ph_past_present_future = {\"Median 2021\": np.median(prior_data.pcs_m.values), \"Median 2022\": np.median(lh_data.pcs_m.values), \"Expected median 2024\":median_2024}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "code_index = 2\n",
    "\n",
    "this_code = code_groups[code_index]\n",
    "attribute_index = 2\n",
    "this_attribute = attribute_columns[attribute_index]\n",
    "start, end = \"2015-11-15\", \"2021-05-31\"\n",
    "\n",
    "prior_data = g_res[(g_res.Project == 'Training')&(g_res.code == this_code)]\n",
    "lh_data = g_res[(g_res.Project == 'Testing')&(g_res.code == this_code)]\n",
    "lh_locations = lh_data.slug.unique()\n",
    "\n",
    "lh_vals = lh_data[\"pcs_m\"].values\n",
    "post_quants = np.quantile(lh_vals, some_quants)\n",
    "prior_vals = g_res[g_res.code == this_code][\"pcs_m\"].values\n",
    "prior_quants = np.quantile(prior_vals, [.03, .25, .48, .5, .52, .75, .97])\n",
    "\n",
    "prior_args = {\n",
    "    'prior_data':prior_data,\n",
    "    'start': start,\n",
    "    'end': end,\n",
    "    'xrange':xrange,\n",
    "    'uninformed_prior': uninformed_tuple,\n",
    "\n",
    "}\n",
    "\n",
    "grid_prior, beta_prior, prior_k_n, prior_df, pcs = prior_distributions(**prior_args)\n",
    "\n",
    "posterior_args = {\n",
    "    'lh_data':lh_data,\n",
    "    'start': \"2022-01-01\",\n",
    "    'end': \"2022-12-31\",\n",
    "    'un_informed': uninformed_tuple,\n",
    "    'informed_prior': prior_k_n\n",
    "}\n",
    "\n",
    "\n",
    "informed, uninformed, beta_p, lh_pcs = posterior_distribution(**posterior_args)\n",
    "post_df = make_results_df(prior_df, informed, source=\"Informed post\", source_norm=\"Ip_n\")\n",
    "post_df = make_results_df(post_df, uninformed, source=\"Uninformed post\", source_norm=\"Un_n\")\n",
    "\n",
    "results_df = pd.DataFrame({\"pcs/m\":xrange, \"2024\":informed})\n",
    "results_df[\"2024 pmf\"] = results_df[\"2024\"]/results_df[\"2024\"].sum()\n",
    "median_2024 = results_df.loc[results_df[\"2024\"].between(.49, .51), \"pcs/m\"].mean()\n",
    "\n",
    "ppf = predicted_summary(lh_pcs, pcs, prior_quants, median_2024)\n",
    "summaries.append(ppf)\n",
    "dfs.append(results_df)\n",
    "ip_past_present_future = {\"Median 2021\": np.median(prior_data.pcs_m.values), \"Median 2022\": np.median(lh_data.pcs_m.values), \"Expected median 2024\":median_2024}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "code_index = 3\n",
    "\n",
    "this_code = code_groups[code_index]\n",
    "attribute_index = 2\n",
    "this_attribute = attribute_columns[attribute_index]\n",
    "start, end = \"2015-11-15\", \"2021-05-31\"\n",
    "\n",
    "prior_data = g_res[(g_res.Project == 'Training')&(g_res.code == this_code)]\n",
    "lh_data = g_res[(g_res.Project == 'Testing')&(g_res.code == this_code)]\n",
    "lh_locations = lh_data.slug.unique()\n",
    "\n",
    "lh_vals = lh_data[\"pcs_m\"].values\n",
    "post_quants = np.quantile(lh_vals, some_quants)\n",
    "prior_vals = g_res[g_res.code == this_code][\"pcs_m\"].values\n",
    "prior_quants = np.quantile(prior_vals, [.03, .25, .48, .5, .52, .75, .97])\n",
    "\n",
    "prior_args = {\n",
    "    'prior_data':prior_data,\n",
    "    'start': start,\n",
    "    'end': end,\n",
    "    'xrange':xrange,\n",
    "    'uninformed_prior': uninformed_tuple,    \n",
    "}\n",
    "\n",
    "grid_prior, beta_prior, prior_k_n, prior_df, pcs = prior_distributions(**prior_args)\n",
    "\n",
    "posterior_args = {\n",
    "    'lh_data':lh_data,\n",
    "    'start': \"2022-01-01\",\n",
    "    'end': \"2022-12-31\",\n",
    "    'un_informed': uninformed_tuple,\n",
    "    'informed_prior': prior_k_n\n",
    "}\n",
    "\n",
    "\n",
    "informed, uninformed, beta_p,lh_pcs = posterior_distribution(**posterior_args)\n",
    "post_df = make_results_df(prior_df, informed, source=\"Informed post\", source_norm=\"Ip_n\")\n",
    "post_df = make_results_df(post_df, uninformed, source=\"Uninformed post\", source_norm=\"Un_n\")\n",
    "\n",
    "results_df = pd.DataFrame({\"pcs/m\":xrange, \"2024\":informed})\n",
    "results_df[\"2024 pmf\"] = results_df[\"2024\"]/results_df[\"2024\"].sum()\n",
    "median_2024 = results_df.loc[results_df[\"2024\"].between(.45, .55), \"pcs/m\"].mean()\n",
    "\n",
    "ppf = predicted_summary(lh_pcs, pcs, prior_quants, median_2024)\n",
    "summaries.append(ppf)\n",
    "dfs.append(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "code_index = 4\n",
    "\n",
    "this_code = code_groups[code_index]\n",
    "attribute_index = 2\n",
    "this_attribute = attribute_columns[attribute_index]\n",
    "start, end = \"2015-11-15\", \"2021-05-31\"\n",
    "\n",
    "prior_data = g_res[(g_res.Project == 'Training')&(g_res.code == this_code)]\n",
    "lh_data = g_res[(g_res.Project == 'Testing')&(g_res.code == this_code)]\n",
    "lh_slugs = lh_data.slug.unique()\n",
    "\n",
    "lh_vals = lh_data[\"pcs_m\"].values\n",
    "post_quants = np.quantile(lh_vals, some_quants)\n",
    "prior_vals = g_res[g_res.code == this_code][\"pcs_m\"].values\n",
    "prior_quants = np.quantile(prior_vals, some_quants)\n",
    "\n",
    "prior_args = {\n",
    "    'prior_data':prior_data,\n",
    "    'start': start,\n",
    "    'end': end,\n",
    "    'xrange':xrange,\n",
    "    'uninformed_prior': uninformed_tuple,    \n",
    "}\n",
    "\n",
    "grid_prior, beta_prior, prior_k_n, prior_df, pcs = prior_distributions(**prior_args)\n",
    "\n",
    "posterior_args = {\n",
    "    'lh_data':lh_data,\n",
    "    'start': \"2022-01-01\",\n",
    "    'end': \"2022-12-31\",\n",
    "    'un_informed': uninformed_tuple,\n",
    "    'informed_prior': prior_k_n\n",
    "}\n",
    "\n",
    "\n",
    "informed, uninformed, beta_p,lh_pcs = posterior_distribution(**posterior_args)\n",
    "post_df = make_results_df(prior_df, informed, source=\"Informed post\", source_norm=\"Ip_n\")\n",
    "post_df = make_results_df(post_df, uninformed, source=\"Uninformed post\", source_norm=\"Un_n\")\n",
    "\n",
    "results_df = pd.DataFrame({\"pcs/m\":xrange, \"2024\":informed})\n",
    "results_df[\"2024 pmf\"] = results_df[\"2024\"]/results_df[\"2024\"].sum()\n",
    "median_2024 = results_df.loc[results_df[\"2024\"].between(.45, .55), \"pcs/m\"].mean()\n",
    "\n",
    "ppf = predicted_summary(lh_pcs, pcs, prior_quants, median_2024)\n",
    "summaries.append(ppf)\n",
    "dfs.append(results_df)\n",
    "\n",
    "sum_dict = {x:summaries[i][\"pcs/m\"].values for i,x  in enumerate(code_groups)}\n",
    "previsions = pd.DataFrame(sum_dict, index=summaries[0].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "previous_medians = previsions.loc[\"Median 2021\"].round(2).values\n",
    "chance_of_exceeding = [x[x[\"pcs/m\"] == previous_medians[i]][\"2024\"].mean() for i,x in enumerate(dfs)]\n",
    "response_question_2_lake = pd.DataFrame([chance_of_exceeding], columns=code_groups, index=[\"P\"])\n",
    "response_question_2_lake.rename(columns=column_names_groups, inplace=True)\n",
    "rq2=response_question_2_lake.T\n",
    "\n",
    "caption = \"The probability that a survey in 2024 will exceed the median (50%) from 2021\"\n",
    "\n",
    "styled = rq2.style.format(precision=2).set_table_styles(table_large_font).set_caption(caption)\n",
    "glue('reply-question-2', styled, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Previous and expected survey results of objects of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```{admonition} Given the median value for the objects of interest in 2021, what is the chance that a survey in 2022 will exceed this value?\n",
    "\n",
    "|Table 5|\n",
    "|:------------------------|\n",
    "|{glue:}`reply-question-2`|\n",
    "|There were more fragmented plastics identified per/meter in 2022 than 2021 (Figure 7, Table 6). Most municipalities will either experience a slight increase or nothing at all. Locations that have historically high counts of these objects will see the greatest increases. This was hypothesised in the 2021 report, the results from the testing data lend support to the assessement. From table 7 and figure 8 we can see how similar the expected values for 2024 are to 2021.|\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Expected results fragmented plastics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "|       Figure 7, Table 6    | Table 7, Figure 8         |\n",
    "|:------------------------:|:----------------------------:|\n",
    "|{glue:}`gfrags-outlook-2024`| {glue:}`unk-2024-meds`    | \n",
    "|{glue:}`unk-summary`| {glue:}`gfrags-predicted_samples` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Expected results personal consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "__Example: Personal consumption.__ The personal consumption objects are those objects that are likely to be used at the beach by visitors, this includes food and tobaco items. There were less personal consumption objects identified per/meter in 2022 than 2021 (Figure 9, Table 8). This effect should be the most noticeable in communities that have active prevention programs (Table 9, Figure 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "|      Figure 9, Table 8 |   Table 9, Figure 10     |\n",
    "|:----------------------:|:------------------------:|\n",
    "|{glue:}`pc-outlook-2024`| {glue:}`pc-2024-meds`    | \n",
    "|{glue:}`pc-summary`| {glue:}`pc-predicted_samples` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Summary of expected survey results Lake Geneva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "caption = \"Table 10: Previous and expected results and the percent of observed samples from 2022 that were included in either the 50% IQR or the 94% IQR of the predicted values.\"\n",
    "\n",
    "rq3 = previsions.style.format(precision=2).set_table_styles(table_large_font).set_caption(caption)\n",
    "\n",
    "glue(\"lake-rq3\", rq3, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```{admonition}  How do the results from 2022 change the expected survey results going forward??\n",
    "With the exception of fragmented plastics we expect reported beach litter densities to be lower in 2024. The greatest improvement will be concerning the objects of personal consumption, there is evidence of a decline that started in 2018 [Summary comparison 2018 - 2021](https://hammerdirt-analyst.github.io/IQAASL-End-0f-Sampling-2021/slr-iqaasl.html). These objects are often the main focus of anti litter campaigns. The expected low values for the Industrial group may be a case of mistaken identity. We base this on the fact that in 2021 fragmented plastics were 18% of the total, in 2022 that number rose to 50%. This is usually because inexperienced surveyors tend to not differentiate between fragmented items, thus some of the objects would be classified as Industrial or professional get placed under the more general category.\n",
    "\n",
    "There are several item types that can be readily identified as Industrial or Profesional:\n",
    "1. Conduit: PVC or ABS fragments\n",
    "2. Plastic concrete form stops\n",
    "3. Plastic saftey barrier fragments\n",
    "4. Pallette angles\n",
    "5. Industrial sheeting, heavy guage plastic for covering\n",
    "6. Pheromone baits\n",
    "\n",
    "The expected decline in Personal hygiene products is encouraging. However, there is no other reason to support the data. For example we are unaware if the sales of plastic cotton swabs has declined or if there is a proposed ban on these products like in France. We also note that there were no reported _plastic tampon applicators_ indentified in 2022.  This also may be case of lack of experience, given that _plastic tampon applicators_ were found in 40% of the samples in 2021 [Finding one object](https://hammerdirt-analyst.github.io/finding-one-object/chance_of_an_encounter.html#results-and-discussion).\n",
    "\n",
    "|Table 10|\n",
    "|:---------:|\n",
    "|{glue:}`lake-rq3`|\n",
    "|__Legend:__ Unk = Unknown group, PC = personal consumption, Ph = personal hygiene, Rc = recreation, Ip = industrial professional|\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```{admonition}  How do the results from 2022 change the expected survey results going forward??\n",
    "Beach litter density at Saint Sulpice is expected to increase. Given the data from 2022 we expect the survey results to increase from the 2021 levels and the density at Saint Sulpice is expected to be greater than the expected value for the lake in general.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Discussion\n",
    "\n",
    "Potential points of discussion\n",
    "\n",
    "1. The lake samples show a decline but the results at Saint Sulpice show an increase. Could this be because the samples at the lake level came from one sampling group and at Saint Sulpice from another?\n",
    "2. Fragmented plastics are 40% of the objects in 2022 and but hisotrically it is around 20% for the lake. Why is their such a difference?\n",
    "3. Is their other evidence to support the expected decline of personal consumption products found on the beach?\n",
    "4. Were there changes to the water treatment facilities (or processes) that would support an anticipated decline in personal hygiene products in general but an increase at Saint Sulpice? For example there were no tampon applicators identified in 2022 but yet they were identified in 40% of samples from 2015-2021.\n",
    "5. What does the presence of shotgun shells on the beach at Saint Sulpice say about hydrological transport mechanisms in lake?\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "The grid approximations function as we expected. The flat areas in the predicted curves demonstrate the need for a more comprehensive model. However, the observed samples in 2022 are within the 94% HDI of the predictions. In most cases we can account for 80% - 100% of the observed with the prediction. \n",
    "\n",
    "### Next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "This script updated 22/08/2024 in Biel, CH\n",
       "\n",
       "❤️ __what you do everyday:__ *analyst at hammerdirt*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = dt.datetime.now().date().strftime(\"%d/%m/%Y\")\n",
    "where = \"Biel, CH\"\n",
    "\n",
    "my_block = f\"\"\"\n",
    "\n",
    "This script updated {today} in {where}\n",
    "\n",
    "\\u2764\\ufe0f __what you do everyday:__ *analyst at hammerdirt*\n",
    "\"\"\"\n",
    "\n",
    "md(my_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git repo: https://github.com/hammerdirt-analyst/solid-waste-team.git\n",
      "\n",
      "Git branch: main\n",
      "\n",
      "seaborn   : 0.13.2\n",
      "matplotlib: 3.8.4\n",
      "pandas    : 2.2.2\n",
      "numpy     : 1.26.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions -b -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}